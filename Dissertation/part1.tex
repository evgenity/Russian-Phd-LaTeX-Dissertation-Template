\chapter{Применение методов машинного обучения к задачам управления инфраструктурой} \label{chapt1}

\section{Введение}

Теория обучения машин (machine learning, машинное обучение) находится на стыке прикладной статистики, численных методов оптимизации, дискретного анализа, и за последние 50 лет оформилась в самостоятельную математическую дисциплину. Методы машинного обучения составляют основу ещё более молодой дисциплины — интеллектуального анализа данных (data mining) \cite{воронцов2009машинное, marsland2015machineconway2012machine}.

\section{Индустриальные задачи в контексте машинного обучения} \label{sect1_1}
\subsection{Медицина}
Использование машинного обучения в медицине \todo{(написать)}: \cite{shipp2002diffuse} \cite{ye2003predicting}

\subsection{Оптимизация бизнес процессов} \label{sect1_2_1}
Попытаюсь скопировать информацию о литературе и сослаться на нее. 

There has recently been some interest in applying machine learning techniques to support the acquisition and adaptation of workflow models. The different learning algorithms, that have been proposed, share some restrictions, which may prevent them from being used in practice. Approaches applying techniques from grammatical inference are restricted to sequential workflows. Other algorithms allowing concurrency require unique activity nodes. This contribution shows how the basic principle of our previous approach to sequential workflow induction can be generalized, so that it is able to deal with concurrency. It does not require unique activity nodes. The presented approach uses a log-likelihood guided search in the space of workflow models, that starts with a most general workflow model containing unique activity nodes. Two split operators are available for specialization. \cite{Herbst2000}

\section{Машинное обучение в задачах управления инфраструктурой}

\subsection{Методы машинного обучения с подкреплением}

Рассмотрим марковский процесс первого порядка с дискретным временем, в котором  вероятность перехода из состояния $x$ в состояние $x'$ под действием $u$ задается как $p_0(x'|x,u)$.  Далее будем рассматривать эволюцию системы на бесконечном горизонте времени, с дисконтированием целевой функции со временем. Введем функцию вознаграждения, зависящую от текущего состояния и применяемого действия  как $R(x,u)$ и политику $\pi(u|x)$, определяющую вероятность применения действия в зависимости от текущего состояния $x$.

Предполагая, что политика $\pi$ и начальное состояние системы $x_0$ заданы, вероятность найти систему в состоянии $x_t$ на момент времени $t > 0$ задается следующим выражением:

\begin{equation}
    \label{eq:pxxt}
     p_\pi(x_t|x_0; t) = 
     \sum_{u_{0:t−1}, x_{1:t−1}}{
     	\prod_{s=0}^{t-1} {
        	p_0(x_{s+1}|x_s, u_x) \pi(u_s|x_s)
         }
      }
\end{equation}

Таким образом ожидаемое дисконтированное вознаграждение в состоянии $x$, при следовании политике $\pi$ задается следующим выражением:

\begin{equation}
    \label{eq:j_pi}
     J_{\pi}(x) = 
     \sum_{s=0}^{\infty}
       \sum_{u', x'}{
          \pi(u'|x')p_\pi(x'|x; s)R(x', u')\gamma^s
        }
\end{equation}
, где $\gamma$ - коэффициент дисконтирования ($0 < \gamma < 1$) и $p_pi(x'|x, 0) = \delta_{x,x'}$. Функция $J_\pi(x)$ называется функцией полезности и определяет ожидаемые выигрыш (вознаграждение) при следовании политике $\pi$. Цель обучения с подкреплением заключается в поиске политики $\pi$, максимизирующей значение $J_\pi(x)$ для всех значений $x$.

Выражение для $J_\pi(x)$ можно переписать в рекурсивном виде:
\begin{equation}
    \label{eq:j_pi_rec}
    \begin{split}
     J_\pi(x) &= 
     \sum_u {\pi(u|x)R(x,u)} + 
     \sum_{s=1}^{\infty}
       \sum_{u', x'}{
          \pi(u'|x')p_\pi(x'|x; s)R(x', u')\gamma^s 
        } \\
        &= \sum_u {\pi(u|x)} \left( 
        R(x,u) + \gamma \sum_{x'}{
        		p_0(x'|x; u)J_{\pi}(x')
            }
       		\right)
            \end{split}
\end{equation}

Процесс нахождения $J_\pi(x)$ методом последовательных приближений (см. подробнее~\cite{sutton1998reinforcement}) называется оценкой политики (англ. policy evaluation). После нахождения некоторой оценки  $J_\pi(x)$ мы можем предложить новую политику $\pi'$:

\begin{equation}
    \label{eq:new_policy}
    \begin{split}
    & \pi'(u|x) = \delta_{u,u(x)}, \\
    & u(x) = \argmax_u R(x, u) + \gamma \sum_{x'} p_0(x'|x, u)J_\pi(x')
    \end{split}
\end{equation}

Очевидно, что для новой политики $\pi'$ , будет выполняется выражение $J_{\pi'}(x) \geq J_\pi(x), \forall x$. Может быть показано, что итеративный процедура $$\pi_0 \rightarrow J_{\pi_0} \rightarrow \pi_1 \rightarrow J_{\pi_1} \rightarrow \pi_2 \rightarrow ... \rightarrow J^*$$ сходится  к оптимальному значению $J^*$.

Отметим, что в рассуждениях выше предполагается, что функции $p_0(x'|x,u)$, $R(x,u)$ заранее известны. Перейдем к ситуации, когда это окружение заранее не задано. В этом случае возможно либо выучить эти функции в процессе исследования и перейти к итеративной процедуре поиска оптимальной политики, либо использовать подходы обучения с подкреплением, не предполагающие априорное наличие модели ($p_0(x'|x,u)$, $R(x,u)$) - TD-Lambda \cite{sutton1998reinforcement} или Q-learning~\cite{Watkins:1989}.

В случае, когда $p_0(x'|x,u)$, $R(x,u)$ не известны, оценку дисконтированного вознаграждения для состояния $x$ ($J_\pi(x)$, см. уравнение (\ref{eq:j_pi})) можно получить при помощи сэмплирования:

\begin{equation}
    \label{eq:j_sampling}
   	J_\pi(x) = (1-\alpha)J_\pi(x) + \alpha(r + \gamma J_\pi(x')).
\end{equation}
, где $x$ - текущее состояние, $x'$ - новое состояние после выбора действия $u$ при следовании политике $\pi(u|x)$ и $r$ - наблюдаемая награда.

Уравнение~(\ref{eq:j_sampling}) соответствует ~\cite{sutton1998reinforcement} алгоритму TD(0). Отметим, что, применяя этот подход, перед выбором новой стратегии $\pi'$ требуется дождаться схождения алгоритма. Если же на каждой итерации менять политику $\pi'$ согласно уравнению (\ref{eq:new_policy}), получается другой подход, известный под названием Actor-Critic (см.~\cite{actor_critic}).

Более элегантный способ вычисления оптимальной политики без модели ($p_0(x'|x,u)$, $R(x,u)$) был предложен \cite{Watkins:1989} К. Ваткинсом. Обозначим как $Q(x, u)$ ожидаемое значение функции полезности состояния $x$ при выполнении действия $u$ и дальнейшем следовании оптимальной политике:

\begin{equation}
    \label{eq:q_learning_definition}
    \begin{split}
      & Q(x, u) = R(x, u) + \gamma \sum_{x'} {
          p_0(x'|x, u) \max_{u'} Q(x', u')
       }
       \\
      & J^{∗}(x) = \max_{u} Q(x,u)
     \end{split}
\end{equation}

В стохастическом окружении и при использовании онлайн-обучения уравнение (\ref{eq:q_learning_definition}) приобретает следующий вид: 

\begin{equation}
    \label{eq:q_learning_online_stochastic}
    Q(x, u) = Q(x, u) + \alpha(R(x, u) + \gamma max_{u'} Q(x', u') − Q(x, u))
\end{equation},
где $\alpha$ - коэффициент, определяющий скорость обучения, $\gamma$ - коэффициент дисконтирования вознаграждений со временем. Далее мы будем придерживаться этих обозначений при формулировании задачи обучения с подкреплением.



\cite{лаптев2011применение, Magnusson:2012:SCW:2351316.2351327, hung2006applying, nelson2008exploiting, arisholm2007data, nguyen2012timely} 
Reinforcement learning is the learning of a mapping from situations to actions so as to maximize a scalar reward or reinforcement signal. The learner is not told which action to take, as in most forms of machine learning, but instead must discover which actions yield the highest reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate's reward, but also the next situation, and through that all subsequent rewards. These two characteristics-trial-and-error search and delayed reward-are the two most important distinguishing features of reinforcement learning. \cite{book:963927, sutton1998introduction}



\subsection{Логические модели, леса решающих деревьев}
Tree boosting is a highly effective and widely used machine learning method. A scalable end-to-end tree boosting system called XGBoost is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. \cite{DBLP:journals/corr/ChenG16} 

Логические модели широко используются \cite{бериков2008современные} для решения задач распознавания и прогнозирования. Это объясняется хорошей интерпретируемостью моделей, имеющих вид логических закономерностей, высокой прогнозирующей способностью, возможностью обрабатывать разнотипные переменные, выделять наиболее важные факторы. Логическую модель можно строить после группировки объектов некоторым алгоритмом, то есть решать задачу распознавания образов в классе логических решающих функций, где под образами понимаются номера кластеров, приписанные объектам. Существуют и алгоритмы, в которых группировка осуществляется непосредственно в ходе построения логической модели. 

\todo{Написать своими словами введение}Machine learning: An artificial intelligence approach\cite{michalski2013machine}

Recently there has been a lot of interest in “ensemble learning” — methods that generate many classifiers and aggregate their results. Two well-known methods are boosting (see, e.g., Shapire et al., 1998) and bagging Breiman (1996) of classification trees. In boosting, successive trees give extra weight to points incorrectly predicted by earlier predictors. In the end, a weighted vote is taken for prediction. In bagging, successive trees do not depend on earlier trees — each is independently constructed using a bootstrap sample of the data set. In the end, a simple majority vote is taken for prediction. Breiman (2001) proposed random forests, which add an additional layer of randomness to bagging. In addition to constructing each tree using a different bootstrap sample of the data, random forests change how the classification or regression trees are constructed. In standard trees, each node is split using the best split among all variables. In a random forest, each node is split using the best among a subset of predictors randomly chosen at that node. This somewhat counterintuitive strategy turns out to perform very well compared to many other classifiers, including discriminant analysis, support vector machines and neural networks, and is robust against overfitting (Breiman, 2001). In addition, it is very user-friendly in the sense that it has only two parameters (the number of variables in the random subset at each node and the number of trees in the forest), and is usually not very sensitive to their values. \cite{liaw2002classification}

%\newpage
%============================================================================================================================

\section{Системы диагностирования технических устройств}
\cite{prishepa}




% Сравнение 
% https://www.quora.com/What-is-the-difference-between-Q-learning-TD-learning-and-TD-lambda/answer/Robby-Goetschalckx?srid=utslN

%\newpage
%============================================================================================================================

\section{Обсуждение}
